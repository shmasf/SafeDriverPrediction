{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "notebooks_path = '../nbs/'\n",
    "input_path = '../input/'\n",
    "output_path = '../output/'\n",
    "print(os.listdir(input_path))\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data set files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(input_path + 'train.csv')\n",
    "test_dataset = pd.read_csv(input_path + 'test.csv')\n",
    "print('Train ({} rows), and test ({} rows) data sets were loaded'.format(train_dataset.shape[0], test_dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Exploration\n",
    "Let's check the size, types  of the train and test data sets.\n",
    "* Data sets size - how many rows and columns each data set contains\n",
    "* What are the data types\n",
    "* Missing values - how many where they are\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting how many columns from each data type we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a table with a count of all columns types \n",
    "def explore_dtypes(df):\n",
    "    types = pd.DataFrame(df.dtypes, columns=['Column Data Type'])\n",
    "    types['Count'] = types.index\n",
    "    types.reset_index(drop=True, inplace=True)\n",
    "    return types.groupby(by=['Column Data Type']).count()\n",
    "    \n",
    "print('Columns count by data type\\n\\nTrain set:')\n",
    "display(explore_dtypes(train_dataset))    \n",
    "\n",
    "print('\\n*************\\nTest set:')\n",
    "display(explore_dtypes(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data contains only columns of types **int64** and **float64**.\n",
    "* Train and test sets data types look the same*. \n",
    "\n",
    " \\*The only diffence is that the train set contains 1 extra column, as expectedwhich is the **'target'** column (labels, type int64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cols = [col for col in train_dataset.columns if col not in test_dataset.columns]\n",
    "print ('Columns that in the train set and not in the test set:', diff_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display('Train', train_dataset.describe())\n",
    "display('-------\\n\\nTest', test_dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Missin Values\n",
    "### Basic exploration of which columns have missing data and in which ratio.\n",
    "Will be usfull for feature exploration and data cleaning.\n",
    "* Replacing -1 with np.nan (Null)\n",
    " * missing values represented as '-1' in all the columns in the data set (taken from the official data description [link][http://sss.sss]).\n",
    " * Need to be replaced to np.nan in order to use pandas built in functions.\n",
    "* Counting missing values by column.\n",
    "* Displaying count and ratio of missing values of each column that has missing values.\n",
    "* Displaying the ratio of missing values for the intire data set.\n",
    "\n",
    "### Conclusions\n",
    "Both train set and test set have simmiller total missing value ratio.\n",
    "\n",
    "~2.4% of the data is missing in each of them.\n",
    "\n",
    "Both train set and test set have missing values in the same columns* and with very simmiller ratio per column.\n",
    "\n",
    "*There is one column,'ps_car_12' , that contains only one missing value, and only in the train set.\n",
    "\n",
    "Some columns only have a small amount of missing values while others have high ratio of nulls.\n",
    "\n",
    "The two most \"problematic\" columns are **'ps_car_o3_cat'**(~69%) and **'ps_car_05_cat'**(~44%) in both train and test sets.\n",
    "\n",
    "\n",
    "------\n",
    "# remove\n",
    "There are 59 columns, 13 of them have missing values (train set).\n",
    "\n",
    "There are 58 columns, 12 of them have missing values (test set).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "def explore_missing_values(ds):\n",
    "    # Replacing '-1' with np.nan (null)\n",
    "    ds.replace(-1, np.nan, inplace= True)\n",
    "\n",
    "    # Counting mising values (nulls) for each column and displaying the columns orderd by nulls count\n",
    "    rows = ds.shape[0] \n",
    "    columns = ds.shape[1]\n",
    "    na_df = pd.DataFrame(ds.isnull().sum(), columns=['Nulls_count'])\n",
    "    na_df['Nulls_ratio'] = np.round((na_df['Nulls_count'] / rows) * 100, 3)\n",
    "    na_df.drop(na_df[na_df['Nulls_count'] == 0].index, axis=0, inplace=True)\n",
    "    na_df.sort_values(by=['Nulls_count'], ascending=False, inplace=True)\n",
    "\n",
    "    # Total missing values count and ratio\n",
    "    nulls_count = ds.isnull().sum().sum()\n",
    "    total_missing_ratio = np.round((nulls_count / (rows * columns)) * 100, 2)\n",
    "    print('There are {} missing values ({}% of the data)'.format(nulls_count, total_missing_ratio))\n",
    "    print('Out of {} columns, {} columns have missing values'.format(columns, len(na_df)))\n",
    "    print('The column \"{}\" has the highest ratio of missing values (~{}%)'.format(na_df.index[0], na_df.iloc[0,1]))\n",
    "    return na_df\n",
    "print('Train set:')\n",
    "nulls_df = explore_missing_values(train_dataset)\n",
    "display(nulls_df)\n",
    "print('****\\n****\\n\\nTest set:')\n",
    "display(explore_missing_values(test_dataset))\n",
    "\n",
    "# Bar chart of training data\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.set_title('Null count - Train data')    \n",
    "\n",
    "# Values that are less than 1% are rounded to 1% for better chart display\n",
    "nulls_df.loc[nulls_df[nulls_df['Nulls_ratio']<1].index, ['Nulls_ratio']] = 1 \n",
    "sns.barplot(x=nulls_df.index, y=nulls_df['Nulls_ratio'], ax=ax)\n",
    "#ax.set_ylabel('Unique values count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Outlier detection and removal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains numerical values, categriacal features and binary features.\n",
    "\n",
    "The type of the feature can be detected by the column name prefix (from data description[link][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_dataset.columns[2:]\n",
    "categorical_features = [col for col in features if '_cat' in col]\n",
    "binary_features = [col for col in features if '_bin' in col]\n",
    "numeric_features = [col for col in features if '_cat' not in col and '_bin' not in col]\n",
    "\n",
    "print('{} Categorical features:\\n{}'.format(len(categorical_features),categorical_features))\n",
    "print('\\n{} Binary features:\\n{}'.format(len(binary_features), binary_features))\n",
    "print('\\n{} Numeric features:\\n{}'.format(len(numeric_features), numeric_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection - Need to check if processing it after filling missing values (nulls) gives better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection \n",
    "from collections import Counter\n",
    "\n",
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v >= n )\n",
    "    \n",
    "    print('There are {0} rows ({2:.2f}%) that contains at least {1} outliers each'.format(len(multiple_outliers), n,100 * len(multiple_outliers)/len(df)))\n",
    "    return multiple_outliers   \n",
    "\n",
    "# detect outliers in numeric_features\n",
    "Outliers_to_drop = detect_outliers(train_dataset.dropna(axis=0), 6,numeric_features)\n",
    "display(train_dataset.ix[Outliers_to_drop, numeric_features])\n",
    "display(train_dataset[numeric_features].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature analysis\n",
    "### Numerical values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix between numerical values (SibSp Parch Age and Fare values) and Survived \n",
    "corr_features = numeric_features[:10].copy()\n",
    "corr_features.append('target')\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "g = sns.heatmap(train_dataset[corr_features].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\", ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a = train_dataset.corr()\n",
    "#a.sort_values(by=['target'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_values(col):\n",
    "    return len(col.value_counts())\n",
    "\n",
    "def get_feature_type(col):\n",
    "    name = col.name\n",
    "    if '_bin' in name or 'target' in name:\n",
    "        f_type = 'binary'\n",
    "    elif '_cat' in name:\n",
    "        f_type = 'categorical'\n",
    "    elif col.dtype == np.int64:\n",
    "        f_type = 'ordinal'\n",
    "    else:\n",
    "        f_type = 'interval'\n",
    "    return f_type\n",
    "\n",
    "def get_feature_group(col):\n",
    "    splited_name = col.name.split('_')\n",
    "    if len(splited_name) > 1:\n",
    "        group = splited_name[1]\n",
    "    else:\n",
    "        group = splited_name[0]\n",
    "    return group\n",
    "\n",
    "unique_values = train_dataset.apply(count_unique_values,axis=0)\n",
    "unique_values = pd.DataFrame(unique_values, columns=['count'])\n",
    "unique_values['d_type'] = train_dataset.dtypes\n",
    "unique_values['f_type'] = train_dataset.apply(get_feature_type, axis=0, reduce=False)\n",
    "unique_values['group'] = train_dataset.apply(get_feature_group, axis=0)\n",
    "unique_values['missing_values'] = train_dataset.isnull().sum()\n",
    "unique_values.drop(['id', 'target'], axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values.sort_values(by=['f_type', 'count', 'group', 'd_type', 'missing_values'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for f_type in ['ordinal', 'categorical', 'interval']:\n",
    "    fig, ax = plt.subplots(figsize=(16,8))\n",
    "    ax.set_title(f_type + ' features')    \n",
    "    ax.set_xlabel('Name')\n",
    "    sliced_data = unique_values[unique_values['f_type'] == f_type].reset_index()\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.barplot(data=sliced_data, x='index', y='count', ax=ax)\n",
    "    ax.set_ylabel('Unique values count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  A better look at features of type interval\n",
    "The chart of these features is not so usefull as the unique values count is in range of 4 up to 70k.<br>In order to keep the chart informative, two extra steps are needed:\n",
    "1. Split the data into 2 charts - features with low / high number of unique values.\n",
    "2. Drop the highest value (ps_car_13 with 70482 unique values) so it is possible to notice the rest of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(nrows=2, ncols=1, figsize=(16,8))\n",
    "ax1.set_title('interval features')    \n",
    "ax1.set_xlabel('Name')\n",
    "sliced_data = unique_values[unique_values['f_type'] == 'interval'].reset_index().sort_values(by='count')\n",
    "sliced_data_start = sliced_data[:-4]\n",
    "sliced_data_end = sliced_data[-4:-1]\n",
    "ax1.set_yticks(sliced_data_start['count'].values)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(data=sliced_data_start, x='index', y='count', ax=ax1)\n",
    "ax1.set_ylabel('Unique values count')\n",
    "\n",
    "ax2.set_yticks(sliced_data_end['count'].values)\n",
    "sns.barplot(data=sliced_data_end, x='index', y='count', ax=ax2, )\n",
    "ax2.set_ylabel('Unique values count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for f_type in unique_values['f_type'].unique():\n",
    "    sliced_features = list(unique_values[unique_values['f_type'] == f_type].index)\n",
    "    sliced_features.append('target')\n",
    "    sliced_data = train_dataset[sliced_features]\n",
    "    fig, ax = plt.subplots(figsize=(16,8))\n",
    "    ax.set_title(f_type + ' features - corrolation')    \n",
    "    sns.heatmap(data=sliced_data.corr(), ax=ax, annot=True, fmt = \".2f\", cmap = \"coolwarm\", linewidths=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_values_with_missing = unique_values[unique_values.missing_values > 0]\n",
    "train_dataset[unique_values_with_missing.index]\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.heatmap(train_dataset[unique_values_with_missing.index].corr(), fmt='.2f', annot=True, linewidths=1, ax=ax)\n",
    "\n",
    "corr = train_dataset.corr()\n",
    "most_corr = {}\n",
    "for col in unique_values_with_missing.index.values:\n",
    "    sorted_corr = corr.sort_values(by=col, ascending=False)\n",
    "    most_corr[col] = sorted_corr[col][1:5].reset_index().values\n",
    "#display(most_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(data={'Name':list(most_corr.keys())})\n",
    "#corr_df['']\n",
    "vals = np.array(list(most_corr.values()))\n",
    "vals[:,1,:]\n",
    "#corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to make a df from it with the top corrolated features for each feature that has missing values\n",
    "# Next sstep is to fill the missing values according to the values of the same feature in rows where the corrolated other features are th smae\n",
    "\n",
    "aa = np.array(list(most_corr.items()))[:,1]\n",
    "aa[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['ps_car_03_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls - \n",
    "print('Data size with nulls', train_dataset.shape)\n",
    "# Drop columns with more than 15% missing values\n",
    "\n",
    "too_many_missing_values_cols = nulls_df[nulls_df.Nulls_ratio > 20].index.values\n",
    "print('Features with more than 20% missing values', too_many_missing_values_cols)\n",
    "\n",
    "train_dataset = train_dataset.drop(too_many_missing_values_cols, axis=1)\n",
    "print('Data size after droppint columns with more than 20% missing values', train_dataset.shape)\n",
    "# Dropping rows with missing values\n",
    "train_dataset = train_dataset.dropna(axis=0)\n",
    "print('Data size after droppint rows that contain missing values', train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features encoding\n",
    "#from sklearn.preprocessing import OneHotEncoder#,LabelEncoder\n",
    "#lbl = LabelEncoder()\n",
    "#ohe = OneHotEncoder()\n",
    "\n",
    "#print(categorical_features)\n",
    "#ohe.fit(train_dataset[['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat']])\n",
    "#ohe.transform(train_dataset[['ps_ind_02_cat']])\n",
    "#display(ohe.transform(train_dataset[['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat']]))\n",
    "#train_dataset[['ps_ind_02_cat']].get_dummies()\n",
    "#a = train_dataset['ps_calc_01']\n",
    "categorical_features = [f for f in train_dataset.columns if '_cat' in f]\n",
    "train_dataset = pd.get_dummies(train_dataset, columns=categorical_features, prefix=categorical_features)\n",
    "#train_dataset[categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = train_dataset[train_dataset.columns[2:]]\n",
    "y = train_dataset[train_dataset.columns[1]]\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=12, shuffle=True)\n",
    "y_train.value_counts(), y_validation.value_counts(), X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "weights = compute_class_weight('balanced', y.unique(), y)\n",
    "scale_pos_weight = weights[1]/weights[0]\n",
    "scale_pos_weight, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(objective='binary:logistic', n_jobs=-1, learning_rate=0.2, gamma=0.007 ,\n",
    "                    n_estimators=30, min_child_weight=5,\n",
    "                    max_depth=3, scale_pos_weight=scale_pos_weight,# subsample=0.8,colsample_bytree=0.8,\n",
    "                    verbose=2)#, eval_metric=['xgb_normalizedgini', 'logloss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb.fit(X_train, y_train ,eval_metric=gini_xgb)\n",
    "print('Finished training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(X_validation)\n",
    "y_pred_prob = xgb.predict_proba(X_validation)\n",
    "\n",
    "\n",
    "y_pred_train = xgb.predict(X_train)\n",
    "y_pred_prob_train = xgb.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(objective='binary:logistic', n_jobs=-1, learning_rate=0.02, gamma=0.007 ,\n",
    "                    n_estimators=100, min_child_weight=5, silent=False, \n",
    "                    max_depth=3, scale_pos_weight=scale_pos_weight,# subsample=0.8,colsample_bytree=0.8,\n",
    "                    verbose=2, eval_metric=['auc'])\n",
    "xgb.fit(X_train, y_train, eval_metric=gini_xgb)\n",
    "print('Finished training')\n",
    "y_pred = xgb.predict(X_validation)\n",
    "y_pred_prob = xgb.predict_proba(X_validation)\n",
    "y_pred_train = xgb.predict(X_train)\n",
    "y_pred_prob_train = xgb.predict_proba(X_train)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "display(confusion_matrix(y_train, y_pred_train))\n",
    "display(classification_report(y_train, y_pred_train))\n",
    "\n",
    "display(confusion_matrix(y_validation, y_pred))\n",
    "display(classification_report(y_validation, y_pred))\n",
    "print(eval_gini(y_validation, y_pred_prob[:, 1]))\n",
    "print(eval_gini(y_train, y_pred_prob_train[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data={'Feature':X_train.columns.values, 'Importance':xgb.feature_importances_}).sort_values(by=['Importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(data={'Feature':X_train.columns.values, 'Importance':xgb.feature_importances_}).sort_values(by=['Importance'], ascending=False)\n",
    "features_to_drop = feature_importances[feature_importances['Importance'] < 0.01]['Feature']\n",
    "features_to_drop\n",
    "train_dataset.drop(features_to_drop.values, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data={'Feature':X_train.columns.values, 'Importance':xgb.feature_importances_}).sort_values(by=['Importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=30, max_depth=8, min_samples_leaf=4, \n",
    "                            max_features=0.2, n_jobs=-1, random_state=0, class_weight='balanced_subsample')\n",
    "rf.fit(X_train, y_train)\n",
    "print('Finished training')\n",
    "y_pred = xgb.predict(X_validation)\n",
    "\n",
    "y_pred_train = xgb.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(confusion_matrix(y_train, y_pred_train))\n",
    "display(classification_report(y_train, y_pred_train))\n",
    "\n",
    "display(confusion_matrix(y_validation, y_pred))\n",
    "display(classification_report(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install numba\n",
    "from numba import jit\n",
    "\n",
    "# Compute gini\n",
    "\n",
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funcitons from olivier's kernel\n",
    "# https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = -eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(objective='binary:logistic',\n",
    "                    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random search with K fold cross validation\n",
    "\n",
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'scale_pos_weight': [scale_pos_weight/10, scale_pos_weight/5, scale_pos_weight],\n",
    "        'learning_rate':[0.01, 0.03, 0.05, 0.1, 0.2],\n",
    "        'n_estimators': [30, 100, 230]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "\n",
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, \n",
    "                                   param_distributions=params,\n",
    "                                   n_iter=param_comb, scoring='roc_auc',\n",
    "                                   n_jobs=-1,\n",
    "                                   cv=skf.split(X,y),\n",
    "                                   verbose=10, random_state=1001, fit_params={'eval_metric':gini_xgb})\n",
    "\n",
    "# Here we go\n",
    "#start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "random_search.fit(X, y)\n",
    "#timer(start_time) # timing ends here for \"start_time\" variabl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
